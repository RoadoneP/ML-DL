{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:6.066200085335175e-10\n",
      "b1:3.756395308839207e-09\n",
      "W2:8.690894735537518e-09\n",
      "b2:1.4012953090392078e-07\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]# 배치 사이즈 선정 0,1,2의 데이터만 불러옴\n",
    "t_batch = t_train[:3]\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch) # 수치 미분 기울기\n",
    "grad_backprop = network.gradient(x_batch, t_batch) # 오차역전파법 기울기\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys(): #딕셔너리의 키값들을 모으는것\n",
    "    #가중치 매개변수의 차이의 절댓값\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n",
    "    # 보면 기울기 차이가 엄청 작다는것은 오차역전파법으로 구한 기울기와 수치 미분으로 구한기울기의\n",
    "    #차이가 매우 적다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터\n",
    "iters_num = 10000 #반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "print(x_train.shape)\n",
    "print(train_size)\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:    0\tTrain acc: 0.13107\tTest acc: 0.13510\tLoss: 2.296889\n",
      "i:  600\tTrain acc: 0.90578\tTest acc: 0.90890\tLoss: 0.270781\n",
      "i: 1200\tTrain acc: 0.92403\tTest acc: 0.92690\tLoss: 0.212668\n",
      "i: 1800\tTrain acc: 0.93620\tTest acc: 0.93740\tLoss: 0.182416\n",
      "i: 2400\tTrain acc: 0.94532\tTest acc: 0.94390\tLoss: 0.104016\n",
      "i: 3000\tTrain acc: 0.95263\tTest acc: 0.95100\tLoss: 0.143427\n",
      "i: 3600\tTrain acc: 0.95767\tTest acc: 0.95570\tLoss: 0.070179\n",
      "i: 4200\tTrain acc: 0.96168\tTest acc: 0.95670\tLoss: 0.061682\n",
      "i: 4800\tTrain acc: 0.96492\tTest acc: 0.96140\tLoss: 0.051916\n",
      "i: 5400\tTrain acc: 0.96792\tTest acc: 0.96270\tLoss: 0.116484\n",
      "i: 6000\tTrain acc: 0.96953\tTest acc: 0.96540\tLoss: 0.122131\n",
      "i: 6600\tTrain acc: 0.97262\tTest acc: 0.96700\tLoss: 0.061088\n",
      "i: 7200\tTrain acc: 0.97445\tTest acc: 0.96750\tLoss: 0.047682\n",
      "i: 7800\tTrain acc: 0.97612\tTest acc: 0.96880\tLoss: 0.058185\n",
      "i: 8400\tTrain acc: 0.97725\tTest acc: 0.96950\tLoss: 0.107972\n",
      "i: 9000\tTrain acc: 0.97883\tTest acc: 0.97000\tLoss: 0.093513\n",
      "i: 9600\tTrain acc: 0.97952\tTest acc: 0.97020\tLoss: 0.062025\n"
     ]
    }
   ],
   "source": [
    "# 에포치 선정\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key] # 오차역전파법으로 구한 기울기를 빼준다.\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('i: {:4d}\\tTrain acc: {:.5f}\\tTest acc: {:.5f}\\tLoss: {:f}'.format(i,train_acc,test_acc,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
